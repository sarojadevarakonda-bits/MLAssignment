{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86035dce",
   "metadata": {},
   "source": [
    "# ML Classification Models - Breast Cancer Dataset Analysis\n",
    "\n",
    "This notebook implements 6 machine learning classification models and evaluates them using multiple metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1e0a3",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e29984",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e199a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset directly from UCI Repository\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# URL from UCI Repository\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "\n",
    "# Load dataset from UCI\n",
    "print(\"Loading dataset from UCI Repository...\")\n",
    "response = requests.get(data_url, verify=False)\n",
    "df = pd.read_csv(io.StringIO(response.text), header=None) \n",
    "print(\"✓ Dataset successfully loaded from UCI!\")\n",
    "\n",
    "# Extract features and target\n",
    "X = df.iloc[:, 2:32].values  # 30 features\n",
    "y = df.iloc[:, 1].map({'M': 1, 'B': 0}).values  # Target: Malignant=1, Benign=0\n",
    "\n",
    "print(\"Dataset Information:\") \n",
    "print(f\"Number of instances: {X.shape[0]}\") \n",
    "print(f\"Number of features: {X.shape[1]}\") \n",
    "print(f\"Target distribution: {np.bincount(y)}\")\n",
    "print(f\"\\nFirst few feature rows:\") \n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c0184",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c811c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_scaled.shape}\")\n",
    "print(f\"Test set size: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e6879",
   "metadata": {},
   "source": [
    "## Step 4: Train Models and Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['Logistic Regression'] = lr_model\n",
    "results['Logistic Regression'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba_lr),\n",
    "    'Precision': precision_score(y_test, y_pred_lr),\n",
    "    'Recall': recall_score(y_test, y_pred_lr),\n",
    "    'F1': f1_score(y_test, y_pred_lr),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred_lr)\n",
    "}\n",
    "print(\"✓ Logistic Regression trained\")\n",
    "\n",
    "# 2. Decision Tree Classifier\n",
    "print(\"Training Decision Tree Classifier...\")\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['Decision Tree'] = dt_model\n",
    "results['Decision Tree'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba_dt),\n",
    "    'Precision': precision_score(y_test, y_pred_dt),\n",
    "    'Recall': recall_score(y_test, y_pred_dt),\n",
    "    'F1': f1_score(y_test, y_pred_dt),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred_dt)\n",
    "}\n",
    "print(\"✓ Decision Tree trained\")\n",
    "\n",
    "# 3. K-Nearest Neighbors\n",
    "print(\"Training K-Nearest Neighbors...\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "y_pred_proba_knn = knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['KNN'] = knn_model\n",
    "results['KNN'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_knn),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba_knn),\n",
    "    'Precision': precision_score(y_test, y_pred_knn),\n",
    "    'Recall': recall_score(y_test, y_pred_knn),\n",
    "    'F1': f1_score(y_test, y_pred_knn),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred_knn)\n",
    "}\n",
    "print(\"✓ KNN trained\")\n",
    "\n",
    "# 4. Naive Bayes (Gaussian)\n",
    "print(\"Training Naive Bayes...\")\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "y_pred_proba_nb = nb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['Naive Bayes'] = nb_model\n",
    "results['Naive Bayes'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_nb),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba_nb),\n",
    "    'Precision': precision_score(y_test, y_pred_nb),\n",
    "    'Recall': recall_score(y_test, y_pred_nb),\n",
    "    'F1': f1_score(y_test, y_pred_nb),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred_nb)\n",
    "}\n",
    "print(\"✓ Naive Bayes trained\")\n",
    "\n",
    "# 5. Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['Random Forest'] = rf_model\n",
    "results['Random Forest'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba_rf),\n",
    "    'Precision': precision_score(y_test, y_pred_rf),\n",
    "    'Recall': recall_score(y_test, y_pred_rf),\n",
    "    'F1': f1_score(y_test, y_pred_rf),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred_rf)\n",
    "}\n",
    "print(\"✓ Random Forest trained\")\n",
    "\n",
    "# 6. XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['XGBoost'] = xgb_model\n",
    "results['XGBoost'] = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    'AUC': roc_auc_score(y_test, y_pred_proba_xgb),\n",
    "    'Precision': precision_score(y_test, y_pred_xgb),\n",
    "    'Recall': recall_score(y_test, y_pred_xgb),\n",
    "    'F1': f1_score(y_test, y_pred_xgb),\n",
    "    'MCC': matthews_corrcoef(y_test, y_pred_xgb)\n",
    "}\n",
    "print(\"✓ XGBoost trained\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All models trained successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c7b7a",
   "metadata": {},
   "source": [
    "## Step 5: Display Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea990a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON TABLE - ML MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*100)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Display as formatted table\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfde4d4",
   "metadata": {},
   "source": [
    "## Step 6: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of all metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    values = results_df[metric].values\n",
    "    models_names = results_df.index\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(models_names)))\n",
    "    \n",
    "    bars = ax.bar(models_names, values, color=colors)\n",
    "    ax.set_title(metric, fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Comparison plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bad83d",
   "metadata": {},
   "source": [
    "## Step 7: Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc762ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "predictions = {\n",
    "    'Logistic Regression': y_pred_lr,\n",
    "    'Decision Tree': y_pred_dt,\n",
    "    'KNN': y_pred_knn,\n",
    "    'Naive Bayes': y_pred_nb,\n",
    "    'Random Forest': y_pred_rf,\n",
    "    'XGBoost': y_pred_xgb\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Confusion Matrices for All Models', fontsize=16, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, y_pred) in enumerate(predictions.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(model_name, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Confusion matrices plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf102d82",
   "metadata": {},
   "source": [
    "## Step 8: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory (relative path - works on any machine)\n",
    "# Navigate from notebooks folder to models folder\n",
    "models_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'models'))\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save scaler\n",
    "with open(os.path.join(models_dir, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save all models\n",
    "for model_name, model in models.items():\n",
    "    filename = os.path.join(models_dir, f'{model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"✓ Saved {model_name}\")\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(models_dir, 'results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"\\n✓ All models and results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7ee45",
   "metadata": {},
   "source": [
    "## Step 9: Model Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE OBSERVATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "observations = {\n",
    "    'Logistic Regression': 'Linear classifier with good baseline performance. Fast training and interpretable coefficients. Suitable for linearly separable problems.',\n",
    "    'Decision Tree': 'Simple non-linear classifier that captures feature interactions. Risk of overfitting without proper pruning. Interpretable decision rules.',\n",
    "    'KNN': 'Instance-based lazy learner. Performance depends on feature scaling and neighborhood size. Computationally expensive for large datasets.',\n",
    "    'Naive Bayes': 'Probabilistic classifier assuming feature independence. Fast and efficient. Performs well with high-dimensional data despite simplifying assumptions.',\n",
    "    'Random Forest': 'Ensemble method that reduces overfitting. Provides feature importance rankings. Best balance of accuracy and interpretability for this dataset.',\n",
    "    'XGBoost': 'Gradient boosting ensemble with regularization. Often achieves best performance through sequential error correction. More prone to overfitting with default parameters.'\n",
    "}\n",
    "\n",
    "for model_name, observation in observations.items():\n",
    "    metrics = results[model_name]\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  Observation: {observation}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
